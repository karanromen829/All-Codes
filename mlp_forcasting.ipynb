{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import os\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data(folder_path, max_rows=200000):\n",
    "    all_data = []\n",
    "    file_count = 0\n",
    "    \n",
    "    # Get all CSV files in the folder\n",
    "    csv_files = glob(os.path.join(folder_path, \"*.csv\"))\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} CSV files in the training folder\")\n",
    "    \n",
    "    for file in csv_files:\n",
    "        # Read CSV\n",
    "        print(f\"Processing file: {os.path.basename(file)}\")\n",
    "        df = pd.read_csv(file)\n",
    "        \n",
    "        # Take only up to max_rows\n",
    "        df = df.head(max_rows)\n",
    "        \n",
    "        # Store the comp_out column\n",
    "        data = df['Pll_out'].values\n",
    "        all_data.append(data)\n",
    "        file_count += 1\n",
    "        print(f\"Processed {file_count}/{len(csv_files)} files\")\n",
    "    \n",
    "    # Combine all data\n",
    "    combined_data = np.concatenate(all_data)\n",
    "    print(f\"Total samples loaded: {len(combined_data)}\")\n",
    "    \n",
    "    # Reshape for MLP input (samples, features)\n",
    "    return combined_data.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp_model():\n",
    "    model = Sequential([\n",
    "        # First Dense layer\n",
    "        Dense(64, input_shape=(1,), activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        # Second Dense layer\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        # Output layer\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    \n",
    "    # Print model summary\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_folder, model_save_path):\n",
    "    # Load training data\n",
    "    print(\"Loading training data...\")\n",
    "    X_train = load_training_data(train_folder)\n",
    "    y_train = X_train.reshape(-1, 1)  # Reshape targets\n",
    "    \n",
    "    print(f\"Training data shape: {X_train.shape}\")\n",
    "    \n",
    "    # Create model\n",
    "    print(\"\\nCreating MLP model...\")\n",
    "    model = create_mlp_model()\n",
    "    \n",
    "    # Define early stopping\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    print(\"\\nStarting model training...\")\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Save model\n",
    "    print(f\"\\nSaving model to {model_save_path}\")\n",
    "    model.save(model_save_path)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training process...\n",
      "Loading training data...\n",
      "Found 90 CSV files in the training folder\n",
      "Processing file: fastN_fastP_3.3_105.csv\n",
      "Processed 1/90 files\n",
      "Processing file: fastN_fastP_3.3_125.csv\n",
      "Processed 2/90 files\n",
      "Processing file: fastN_fastP_3.3_45.csv\n",
      "Processed 3/90 files\n",
      "Processing file: fastN_fastP_3.3_85.csv\n",
      "Processed 4/90 files\n",
      "Processing file: fastN_fastP_3.6_145.csv\n",
      "Processed 5/90 files\n",
      "Processing file: fastN_fastP_3.6_165.csv\n",
      "Processed 6/90 files\n",
      "Processing file: fastN_fastP_3.6_185.csv\n",
      "Processed 7/90 files\n",
      "Processing file: fastN_fastP_3_105.csv\n",
      "Processed 8/90 files\n",
      "Processing file: fastN_fastP_3_125.csv\n",
      "Processed 9/90 files\n",
      "Processing file: fastN_fastP_3_145.csv\n",
      "Processed 10/90 files\n",
      "Processing file: fastN_fastP_3_165.csv\n",
      "Processed 11/90 files\n",
      "Processing file: fastN_fastP_3_85.csv\n",
      "Processed 12/90 files\n",
      "Processing file: fastN_slowP_3.3_105.csv\n",
      "Processed 13/90 files\n",
      "Processing file: fastN_slowP_3.3_125.csv\n",
      "Processed 14/90 files\n",
      "Processing file: fastN_slowP_3.3_145.csv\n",
      "Processed 15/90 files\n",
      "Processing file: fastN_slowP_3.3_165.csv\n",
      "Processed 16/90 files\n",
      "Processing file: fastN_slowP_3.3_185.csv\n",
      "Processed 17/90 files\n",
      "Processing file: fastN_slowP_3.3_25.csv\n",
      "Processed 18/90 files\n",
      "Processing file: fastN_slowP_3.3_45.csv\n",
      "Processed 19/90 files\n",
      "Processing file: fastN_slowP_3.3_65.csv\n",
      "Processed 20/90 files\n",
      "Processing file: fastN_slowP_3.3_85.csv\n",
      "Processed 21/90 files\n",
      "Processing file: fastN_slowP_3.6_105.csv\n",
      "Processed 22/90 files\n",
      "Processing file: fastN_slowP_3.6_125.csv\n",
      "Processed 23/90 files\n",
      "Processing file: fastN_slowP_3.6_145.csv\n",
      "Processed 24/90 files\n",
      "Processing file: fastN_slowP_3.6_165.csv\n",
      "Processed 25/90 files\n",
      "Processing file: fastN_slowP_3.6_25.csv\n",
      "Processed 26/90 files\n",
      "Processing file: fastN_slowP_3.6_45.csv\n",
      "Processed 27/90 files\n",
      "Processing file: fastN_slowP_3.6_65.csv\n",
      "Processed 28/90 files\n",
      "Processing file: fastN_slowP_3.6_85.csv\n",
      "Processed 29/90 files\n",
      "Processing file: fastN_slowP_3_105.csv\n",
      "Processed 30/90 files\n",
      "Processing file: fastN_slowP_3_125.csv\n",
      "Processed 31/90 files\n",
      "Processing file: fastN_slowP_3_145.csv\n",
      "Processed 32/90 files\n",
      "Processing file: fastN_slowP_3_165.csv\n",
      "Processed 33/90 files\n",
      "Processing file: fastN_slowP_3_185.csv\n",
      "Processed 34/90 files\n",
      "Processing file: fastN_slowP_3_45.csv\n",
      "Processed 35/90 files\n",
      "Processing file: fastN_slowP_3_65.csv\n",
      "Processed 36/90 files\n",
      "Processing file: fastN_slowP_3_85.csv\n",
      "Processed 37/90 files\n",
      "Processing file: slowN_slowP_3.3_105.csv\n",
      "Processed 38/90 files\n",
      "Processing file: slowN_slowP_3.3_125.csv\n",
      "Processed 39/90 files\n",
      "Processing file: slowN_slowP_3.3_145.csv\n",
      "Processed 40/90 files\n",
      "Processing file: slowN_slowP_3.3_165.csv\n",
      "Processed 41/90 files\n",
      "Processing file: slowN_slowP_3.3_185.csv\n",
      "Processed 42/90 files\n",
      "Processing file: slowN_slowP_3.3_25.csv\n",
      "Processed 43/90 files\n",
      "Processing file: slowN_slowP_3.3_45.csv\n",
      "Processed 44/90 files\n",
      "Processing file: slowN_slowP_3.3_65.csv\n",
      "Processed 45/90 files\n",
      "Processing file: slowN_slowP_3.3_85.csv\n",
      "Processed 46/90 files\n",
      "Processing file: slowN_slowP_3.6_105.csv\n",
      "Processed 47/90 files\n",
      "Processing file: slowN_slowP_3.6_125.csv\n",
      "Processed 48/90 files\n",
      "Processing file: slowN_slowP_3.6_145.csv\n",
      "Processed 49/90 files\n",
      "Processing file: slowN_slowP_3.6_165.csv\n",
      "Processed 50/90 files\n",
      "Processing file: slowN_slowP_3.6_25.csv\n",
      "Processed 51/90 files\n",
      "Processing file: slowN_slowP_3.6_45.csv\n",
      "Processed 52/90 files\n",
      "Processing file: slowN_slowP_3.6_5.csv\n",
      "Processed 53/90 files\n",
      "Processing file: slowN_slowP_3.6_65.csv\n",
      "Processed 54/90 files\n",
      "Processing file: slowN_slowP_3.6_85.csv\n",
      "Processed 55/90 files\n",
      "Processing file: slowN_slowP_3_105.csv\n",
      "Processed 56/90 files\n",
      "Processing file: slowN_slowP_3_125.csv\n",
      "Processed 57/90 files\n",
      "Processing file: slowN_slowP_3_145.csv\n",
      "Processed 58/90 files\n",
      "Processing file: slowN_slowP_3_165.csv\n",
      "Processed 59/90 files\n",
      "Processing file: slowN_slowP_3_185.csv\n",
      "Processed 60/90 files\n",
      "Processing file: slowN_slowP_3_65.csv\n",
      "Processed 61/90 files\n",
      "Processing file: slowN_slowP_3_85.csv\n",
      "Processed 62/90 files\n",
      "Processing file: typical_3.3_105.csv\n",
      "Processed 63/90 files\n",
      "Processing file: typical_3.3_125.csv\n",
      "Processed 64/90 files\n",
      "Processing file: typical_3.3_145.csv\n",
      "Processed 65/90 files\n",
      "Processing file: typical_3.3_165.csv\n",
      "Processed 66/90 files\n",
      "Processing file: typical_3.3_185.csv\n",
      "Processed 67/90 files\n",
      "Processing file: typical_3.3_25.csv\n",
      "Processed 68/90 files\n",
      "Processing file: typical_3.3_45.csv\n",
      "Processed 69/90 files\n",
      "Processing file: typical_3.3_5.csv\n",
      "Processed 70/90 files\n",
      "Processing file: typical_3.3_65.csv\n",
      "Processed 71/90 files\n",
      "Processing file: typical_3.3_85.csv\n",
      "Processed 72/90 files\n",
      "Processing file: typical_3.6_105.csv\n",
      "Processed 73/90 files\n",
      "Processing file: typical_3.6_125.csv\n",
      "Processed 74/90 files\n",
      "Processing file: typical_3.6_145.csv\n",
      "Processed 75/90 files\n",
      "Processing file: typical_3.6_165.csv\n",
      "Processed 76/90 files\n",
      "Processing file: typical_3.6_25.csv\n",
      "Processed 77/90 files\n",
      "Processing file: typical_3.6_45.csv\n",
      "Processed 78/90 files\n",
      "Processing file: typical_3.6_5.csv\n",
      "Processed 79/90 files\n",
      "Processing file: typical_3.6_65.csv\n",
      "Processed 80/90 files\n",
      "Processing file: typical_3.6_85.csv\n",
      "Processed 81/90 files\n",
      "Processing file: typical_3_105.csv\n",
      "Processed 82/90 files\n",
      "Processing file: typical_3_125.csv\n",
      "Processed 83/90 files\n",
      "Processing file: typical_3_145.csv\n",
      "Processed 84/90 files\n",
      "Processing file: typical_3_165.csv\n",
      "Processed 85/90 files\n",
      "Processing file: typical_3_185.csv\n",
      "Processed 86/90 files\n",
      "Processing file: typical_3_25.csv\n",
      "Processed 87/90 files\n",
      "Processing file: typical_3_45.csv\n",
      "Processed 88/90 files\n",
      "Processing file: typical_3_65.csv\n",
      "Processed 89/90 files\n",
      "Processing file: typical_3_85.csv\n",
      "Processed 90/90 files\n",
      "Total samples loaded: 9000000\n",
      "Training data shape: (9000000, 1)\n",
      "\n",
      "Creating MLP model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m128\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,241</span> (8.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,241\u001b[0m (8.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,241</span> (8.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,241\u001b[0m (8.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting model training...\n",
      "Epoch 1/50\n",
      "\u001b[1m225000/225000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 623us/step - loss: 0.0438 - mae: 0.1521 - val_loss: 0.0029 - val_mae: 0.0414\n",
      "Epoch 2/50\n",
      "\u001b[1m225000/225000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 629us/step - loss: 0.0366 - mae: 0.1472 - val_loss: 0.0030 - val_mae: 0.0400\n",
      "Epoch 3/50\n",
      "\u001b[1m225000/225000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 639us/step - loss: 0.0365 - mae: 0.1470 - val_loss: 0.0012 - val_mae: 0.0315\n",
      "Epoch 4/50\n",
      "\u001b[1m225000/225000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 620us/step - loss: 0.0365 - mae: 0.1468 - val_loss: 0.0012 - val_mae: 0.0295\n",
      "Epoch 5/50\n",
      "\u001b[1m225000/225000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 612us/step - loss: 0.0365 - mae: 0.1469 - val_loss: 0.0016 - val_mae: 0.0334\n",
      "Epoch 6/50\n",
      "\u001b[1m225000/225000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 613us/step - loss: 0.0365 - mae: 0.1468 - val_loss: 0.0011 - val_mae: 0.0280\n",
      "Epoch 7/50\n",
      "\u001b[1m225000/225000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 621us/step - loss: 0.0364 - mae: 0.1466 - val_loss: 0.0019 - val_mae: 0.0377\n",
      "Epoch 8/50\n",
      "\u001b[1m225000/225000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 612us/step - loss: 0.0365 - mae: 0.1468 - val_loss: 9.4562e-04 - val_mae: 0.0283\n",
      "Epoch 9/50\n",
      "\u001b[1m225000/225000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 621us/step - loss: 0.0364 - mae: 0.1466 - val_loss: 0.0026 - val_mae: 0.0452\n",
      "Epoch 10/50\n",
      "\u001b[1m225000/225000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 613us/step - loss: 0.0365 - mae: 0.1467 - val_loss: 9.1350e-04 - val_mae: 0.0204\n",
      "Epoch 11/50\n",
      "\u001b[1m225000/225000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 615us/step - loss: 0.0364 - mae: 0.1467 - val_loss: 0.0020 - val_mae: 0.0375\n",
      "Epoch 12/50\n",
      "\u001b[1m225000/225000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 621us/step - loss: 0.0365 - mae: 0.1466 - val_loss: 9.2153e-04 - val_mae: 0.0227\n",
      "Epoch 13/50\n",
      "\u001b[1m225000/225000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 613us/step - loss: 0.0365 - mae: 0.1467 - val_loss: 0.0014 - val_mae: 0.0354\n",
      "Epoch 14/50\n",
      "\u001b[1m225000/225000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 617us/step - loss: 0.0365 - mae: 0.1468 - val_loss: 0.0011 - val_mae: 0.0267\n",
      "Epoch 15/50\n",
      "\u001b[1m225000/225000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 614us/step - loss: 0.0365 - mae: 0.1467 - val_loss: 9.2248e-04 - val_mae: 0.0259\n",
      "Epoch 15: early stopping\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "\n",
      "Saving model to E:\\PLL\\forcast\\forcast_with_mlp\\result\\model.keras\n",
      "Training completed!\n",
      "\n",
      "Final Training Metrics:\n",
      "Loss: 0.0364\n",
      "Validation Loss: 0.0009\n",
      "MAE: 0.1466\n",
      "Validation MAE: 0.0259\n"
     ]
    }
   ],
   "source": [
    "# Set paths\n",
    "train_folder = \"E:\\\\PLL\\\\train\"  # Adjust this path\n",
    "model_save_path = \"E:\\\\PLL\\\\forcast\\\\forcast_with_mlp\\\\result\\\\model.keras\"  # Adjust this path\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training process...\")\n",
    "model, history = train_model(train_folder, model_save_path)\n",
    "print(\"Training completed!\")\n",
    "\n",
    "# Print final metrics\n",
    "final_loss = history.history['loss'][-1]\n",
    "final_val_loss = history.history['val_loss'][-1]\n",
    "final_mae = history.history['mae'][-1]\n",
    "final_val_mae = history.history['val_mae'][-1]\n",
    "\n",
    "print(\"\\nFinal Training Metrics:\")\n",
    "print(f\"Loss: {final_loss:.4f}\")\n",
    "print(f\"Validation Loss: {final_val_loss:.4f}\")\n",
    "print(f\"MAE: {final_mae:.4f}\")\n",
    "print(f\"Validation MAE: {final_val_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fastN_fastP_3_185.csv...\n",
      "\u001b[1m7292/7292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 281us/step\n",
      "Processing fastN_slowP_3.6_185.csv...\n",
      "\u001b[1m7292/7292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 277us/step\n",
      "Processing fastN_slowP_3_25.csv...\n",
      "\u001b[1m7292/7292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 275us/step\n",
      "Processing slowN_slowP_3.6_185.csv...\n",
      "\u001b[1m7292/7292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 271us/step\n",
      "Processing typical_3.6_185.csv...\n",
      "\u001b[1m7292/7292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 276us/step\n",
      "Processing typical_3_5.csv...\n",
      "\u001b[1m7292/7292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 271us/step\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "def calculate_snr(signal, noise):\n",
    "    \"\"\"\n",
    "    Calculate the Signal-to-Noise Ratio (SNR) in dB.\n",
    "    \"\"\"\n",
    "    signal_power = np.mean(np.square(signal))\n",
    "    noise_power = np.mean(np.square(noise))\n",
    "    \n",
    "    if noise_power == 0:\n",
    "        return float('inf')  # Avoid division by zero\n",
    "    \n",
    "    return 10 * np.log10(signal_power / noise_power)\n",
    "\n",
    "def analyze_test_file(file_path, model_path, output_folder):\n",
    "    \"\"\"\n",
    "    Analyze a single test file, make predictions, and create visualizations\n",
    "    \"\"\"\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Split into actual values\n",
    "    actual_train = df['Pll_out'][:200000].values\n",
    "    actual_test = df['Pll_out'][200000:].values\n",
    "    \n",
    "    # Prepare data for prediction\n",
    "    X_test = actual_test.reshape(-1, 1)\n",
    "    \n",
    "    # Load model and make predictions\n",
    "    model = load_model(model_path)\n",
    "    predictions = model.predict(X_test).flatten()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(actual_test, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(actual_test, predictions)\n",
    "    r2 = r2_score(actual_test, predictions)\n",
    "    snr = calculate_snr(actual_test, actual_test - predictions)\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Plot actual values (first 200000)\n",
    "    plt.plot(range(len(actual_train)), \n",
    "             actual_train, \n",
    "             label='Actual (First 200000)', \n",
    "             color='blue')\n",
    "    \n",
    "    # Plot predictions\n",
    "    plt.plot(range(len(actual_train), len(actual_train) + len(predictions)), \n",
    "             predictions, \n",
    "             label='Predictions', \n",
    "             color='red') \n",
    "             #, linestyle='--')\n",
    "    \n",
    "    # Add metrics to plot\n",
    "    metrics_text = f'Metrics (After 200000):\\nRMSE: {rmse:.2f}\\nMAE: {mae:.2f}\\nR²: {r2:.2f}\\nSNR: {snr:.2f} dB'\n",
    "    plt.text(0.02, 0.98, metrics_text,\n",
    "             transform=plt.gca().transAxes,\n",
    "             verticalalignment='top',\n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.title(f'Actual vs Predicted Values\\nFile: {os.path.basename(file_path)}')\n",
    "    plt.xlabel('Row Number')\n",
    "    plt.ylabel('Pll_out Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Save plot\n",
    "    plot_filename = os.path.join(output_folder, \n",
    "                                f'plot_{os.path.basename(file_path).replace(\".csv\", \".png\")}')\n",
    "    plt.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Return metrics\n",
    "    return {\n",
    "        'file': os.path.basename(file_path),\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2,\n",
    "        'snr': snr\n",
    "    }\n",
    "\n",
    "def process_all_test_files(test_folder, model_path, output_folder):\n",
    "    \"\"\"\n",
    "    Process all test files and save results\n",
    "    \"\"\"\n",
    "    # Create output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Get all CSV files\n",
    "    test_files = glob(os.path.join(test_folder, \"*.csv\"))\n",
    "    \n",
    "    # Process each file and collect metrics\n",
    "    all_metrics = []\n",
    "    for file in test_files:\n",
    "        print(f\"Processing {os.path.basename(file)}...\")\n",
    "        metrics = analyze_test_file(file, model_path, output_folder)\n",
    "        all_metrics.append(metrics)\n",
    "    \n",
    "    # Create metrics summary\n",
    "    metrics_df = pd.DataFrame(all_metrics)\n",
    "    metrics_filepath = os.path.join(output_folder, 'metrics_summary.csv')\n",
    "    metrics_df.to_csv(metrics_filepath, index=False)\n",
    "    \n",
    "    # Create summary plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    files = metrics_df['file']\n",
    "    x = range(len(files))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.bar(x, metrics_df['rmse'], alpha=0.6)\n",
    "    plt.title('RMSE by File')\n",
    "    plt.xticks(x, files, rotation=45)\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.bar(x, metrics_df['r2'], alpha=0.6)\n",
    "    plt.title('R² Score by File')\n",
    "    plt.xticks(x, files, rotation=45)\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.bar(x, metrics_df['snr'], alpha=0.6)\n",
    "    plt.title('SNR by File')\n",
    "    plt.xticks(x, files, rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_folder, 'metrics_summary.png'), \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Usage example for Colab\n",
    "if __name__ == \"__main__\":\n",
    "    # Define paths (adjust these for your Colab setup)\n",
    "    test_folder = \"E:\\\\PLL\\\\test\"\n",
    "    model_path = \"E:\\\\PLL\\\\forcast\\\\forcast_with_mlp\\\\result\\\\model.keras\"\n",
    "    output_folder = 'E:\\\\PLL\\\\forcast\\\\forcast_with_mlp\\\\result'\n",
    "    \n",
    "    # Process all test files\n",
    "    process_all_test_files(test_folder, model_path, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
