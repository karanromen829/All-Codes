{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import os\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data(folder_path, max_rows=60000):\n",
    "   \n",
    "    all_data = []\n",
    "    file_count = 0\n",
    "    \n",
    "    # Get all CSV files in the folder\n",
    "    csv_files = glob(os.path.join(folder_path, \"*.csv\"))\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} CSV files in the training folder\")\n",
    "    \n",
    "    for file in csv_files:\n",
    "        # Read CSV\n",
    "        print(f\"Processing file: {os.path.basename(file)}\")\n",
    "        df = pd.read_csv(file)\n",
    "        \n",
    "        # Take only up to max_rows\n",
    "        df = df.head(max_rows)\n",
    "        \n",
    "        # Store the Pll_out column\n",
    "        data = df['Pll_out'].values\n",
    "        all_data.append(data)\n",
    "        file_count += 1\n",
    "        print(f\"Processed {file_count}/{len(csv_files)} files\")\n",
    "    \n",
    "    # Combine all data\n",
    "    combined_data = np.concatenate(all_data)\n",
    "    print(f\"Total samples loaded: {len(combined_data)}\")\n",
    "    \n",
    "    # Reshape for LSTM input (samples, timestamps, features)\n",
    "    return combined_data.reshape(-1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model():\n",
    "   \n",
    "    model = Sequential([\n",
    "        # First LSTM layer with return sequences\n",
    "        LSTM(64, input_shape=(1, 1), return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        # Second LSTM layer\n",
    "        LSTM(32),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        # Dense layers\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    \n",
    "    # Print model summary\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_folder, model_save_path):\n",
    "    \n",
    "    # Load training data\n",
    "    print(\"Loading training data...\")\n",
    "    X_train = load_training_data(train_folder)\n",
    "    y_train = X_train.reshape(-1, 1)  # Reshape targets\n",
    "    \n",
    "    print(f\"Training data shape: {X_train.shape}\")\n",
    "    \n",
    "    # Create model\n",
    "    print(\"\\nCreating LSTM model...\")\n",
    "    model = create_lstm_model()\n",
    "    \n",
    "    # Define early stopping\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    print(\"\\nStarting model training...\")\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=1,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Save model\n",
    "    print(f\"\\nSaving model to {model_save_path}\")\n",
    "    model.save(model_save_path)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training process...\n",
      "Loading training data...\n",
      "Found 90 CSV files in the training folder\n",
      "Processing file: fastN_fastP_3.3_105.csv\n",
      "Processed 1/90 files\n",
      "Processing file: fastN_fastP_3.3_125.csv\n",
      "Processed 2/90 files\n",
      "Processing file: fastN_fastP_3.3_45.csv\n",
      "Processed 3/90 files\n",
      "Processing file: fastN_fastP_3.3_85.csv\n",
      "Processed 4/90 files\n",
      "Processing file: fastN_fastP_3.6_145.csv\n",
      "Processed 5/90 files\n",
      "Processing file: fastN_fastP_3.6_165.csv\n",
      "Processed 6/90 files\n",
      "Processing file: fastN_fastP_3.6_185.csv\n",
      "Processed 7/90 files\n",
      "Processing file: fastN_fastP_3_105.csv\n",
      "Processed 8/90 files\n",
      "Processing file: fastN_fastP_3_125.csv\n",
      "Processed 9/90 files\n",
      "Processing file: fastN_fastP_3_145.csv\n",
      "Processed 10/90 files\n",
      "Processing file: fastN_fastP_3_165.csv\n",
      "Processed 11/90 files\n",
      "Processing file: fastN_fastP_3_85.csv\n",
      "Processed 12/90 files\n",
      "Processing file: fastN_slowP_3.3_105.csv\n",
      "Processed 13/90 files\n",
      "Processing file: fastN_slowP_3.3_125.csv\n",
      "Processed 14/90 files\n",
      "Processing file: fastN_slowP_3.3_145.csv\n",
      "Processed 15/90 files\n",
      "Processing file: fastN_slowP_3.3_165.csv\n",
      "Processed 16/90 files\n",
      "Processing file: fastN_slowP_3.3_185.csv\n",
      "Processed 17/90 files\n",
      "Processing file: fastN_slowP_3.3_25.csv\n",
      "Processed 18/90 files\n",
      "Processing file: fastN_slowP_3.3_45.csv\n",
      "Processed 19/90 files\n",
      "Processing file: fastN_slowP_3.3_65.csv\n",
      "Processed 20/90 files\n",
      "Processing file: fastN_slowP_3.3_85.csv\n",
      "Processed 21/90 files\n",
      "Processing file: fastN_slowP_3.6_105.csv\n",
      "Processed 22/90 files\n",
      "Processing file: fastN_slowP_3.6_125.csv\n",
      "Processed 23/90 files\n",
      "Processing file: fastN_slowP_3.6_145.csv\n",
      "Processed 24/90 files\n",
      "Processing file: fastN_slowP_3.6_165.csv\n",
      "Processed 25/90 files\n",
      "Processing file: fastN_slowP_3.6_25.csv\n",
      "Processed 26/90 files\n",
      "Processing file: fastN_slowP_3.6_45.csv\n",
      "Processed 27/90 files\n",
      "Processing file: fastN_slowP_3.6_65.csv\n",
      "Processed 28/90 files\n",
      "Processing file: fastN_slowP_3.6_85.csv\n",
      "Processed 29/90 files\n",
      "Processing file: fastN_slowP_3_105.csv\n",
      "Processed 30/90 files\n",
      "Processing file: fastN_slowP_3_125.csv\n",
      "Processed 31/90 files\n",
      "Processing file: fastN_slowP_3_145.csv\n",
      "Processed 32/90 files\n",
      "Processing file: fastN_slowP_3_165.csv\n",
      "Processed 33/90 files\n",
      "Processing file: fastN_slowP_3_185.csv\n",
      "Processed 34/90 files\n",
      "Processing file: fastN_slowP_3_45.csv\n",
      "Processed 35/90 files\n",
      "Processing file: fastN_slowP_3_65.csv\n",
      "Processed 36/90 files\n",
      "Processing file: fastN_slowP_3_85.csv\n",
      "Processed 37/90 files\n",
      "Processing file: slowN_slowP_3.3_105.csv\n",
      "Processed 38/90 files\n",
      "Processing file: slowN_slowP_3.3_125.csv\n",
      "Processed 39/90 files\n",
      "Processing file: slowN_slowP_3.3_145.csv\n",
      "Processed 40/90 files\n",
      "Processing file: slowN_slowP_3.3_165.csv\n",
      "Processed 41/90 files\n",
      "Processing file: slowN_slowP_3.3_185.csv\n",
      "Processed 42/90 files\n",
      "Processing file: slowN_slowP_3.3_25.csv\n",
      "Processed 43/90 files\n",
      "Processing file: slowN_slowP_3.3_45.csv\n",
      "Processed 44/90 files\n",
      "Processing file: slowN_slowP_3.3_65.csv\n",
      "Processed 45/90 files\n",
      "Processing file: slowN_slowP_3.3_85.csv\n",
      "Processed 46/90 files\n",
      "Processing file: slowN_slowP_3.6_105.csv\n",
      "Processed 47/90 files\n",
      "Processing file: slowN_slowP_3.6_125.csv\n",
      "Processed 48/90 files\n",
      "Processing file: slowN_slowP_3.6_145.csv\n",
      "Processed 49/90 files\n",
      "Processing file: slowN_slowP_3.6_165.csv\n",
      "Processed 50/90 files\n",
      "Processing file: slowN_slowP_3.6_25.csv\n",
      "Processed 51/90 files\n",
      "Processing file: slowN_slowP_3.6_45.csv\n",
      "Processed 52/90 files\n",
      "Processing file: slowN_slowP_3.6_5.csv\n",
      "Processed 53/90 files\n",
      "Processing file: slowN_slowP_3.6_65.csv\n",
      "Processed 54/90 files\n",
      "Processing file: slowN_slowP_3.6_85.csv\n",
      "Processed 55/90 files\n",
      "Processing file: slowN_slowP_3_105.csv\n",
      "Processed 56/90 files\n",
      "Processing file: slowN_slowP_3_125.csv\n",
      "Processed 57/90 files\n",
      "Processing file: slowN_slowP_3_145.csv\n",
      "Processed 58/90 files\n",
      "Processing file: slowN_slowP_3_165.csv\n",
      "Processed 59/90 files\n",
      "Processing file: slowN_slowP_3_185.csv\n",
      "Processed 60/90 files\n",
      "Processing file: slowN_slowP_3_65.csv\n",
      "Processed 61/90 files\n",
      "Processing file: slowN_slowP_3_85.csv\n",
      "Processed 62/90 files\n",
      "Processing file: typical_3.3_105.csv\n",
      "Processed 63/90 files\n",
      "Processing file: typical_3.3_125.csv\n",
      "Processed 64/90 files\n",
      "Processing file: typical_3.3_145.csv\n",
      "Processed 65/90 files\n",
      "Processing file: typical_3.3_165.csv\n",
      "Processed 66/90 files\n",
      "Processing file: typical_3.3_185.csv\n",
      "Processed 67/90 files\n",
      "Processing file: typical_3.3_25.csv\n",
      "Processed 68/90 files\n",
      "Processing file: typical_3.3_45.csv\n",
      "Processed 69/90 files\n",
      "Processing file: typical_3.3_5.csv\n",
      "Processed 70/90 files\n",
      "Processing file: typical_3.3_65.csv\n",
      "Processed 71/90 files\n",
      "Processing file: typical_3.3_85.csv\n",
      "Processed 72/90 files\n",
      "Processing file: typical_3.6_105.csv\n",
      "Processed 73/90 files\n",
      "Processing file: typical_3.6_125.csv\n",
      "Processed 74/90 files\n",
      "Processing file: typical_3.6_145.csv\n",
      "Processed 75/90 files\n",
      "Processing file: typical_3.6_165.csv\n",
      "Processed 76/90 files\n",
      "Processing file: typical_3.6_25.csv\n",
      "Processed 77/90 files\n",
      "Processing file: typical_3.6_45.csv\n",
      "Processed 78/90 files\n",
      "Processing file: typical_3.6_5.csv\n",
      "Processed 79/90 files\n",
      "Processing file: typical_3.6_65.csv\n",
      "Processed 80/90 files\n",
      "Processing file: typical_3.6_85.csv\n",
      "Processed 81/90 files\n",
      "Processing file: typical_3_105.csv\n",
      "Processed 82/90 files\n",
      "Processing file: typical_3_125.csv\n",
      "Processed 83/90 files\n",
      "Processing file: typical_3_145.csv\n",
      "Processed 84/90 files\n",
      "Processing file: typical_3_165.csv\n",
      "Processed 85/90 files\n",
      "Processing file: typical_3_185.csv\n",
      "Processed 86/90 files\n",
      "Processing file: typical_3_25.csv\n",
      "Processed 87/90 files\n",
      "Processing file: typical_3_45.csv\n",
      "Processed 88/90 files\n",
      "Processing file: typical_3_65.csv\n",
      "Processed 89/90 files\n",
      "Processing file: typical_3_85.csv\n",
      "Processed 90/90 files\n",
      "Total samples loaded: 5400000\n",
      "Training data shape: (5400000, 1, 1)\n",
      "\n",
      "Creating LSTM model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Karan kumar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │        \u001b[38;5;34m16,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │        \u001b[38;5;34m12,416\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m528\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">29,857</span> (116.63 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m29,857\u001b[0m (116.63 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">29,857</span> (116.63 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m29,857\u001b[0m (116.63 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting model training...\n",
      "\u001b[1m135000/135000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m479s\u001b[0m 4ms/step - loss: 0.0165 - mae: 0.0527 - val_loss: 0.0058 - val_mae: 0.0576\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Saving model to D:\\PLL project\\LSTM\\Forcasting\\for_all_files\\Aman_code\\result\\/model.keras\n",
      "Training completed!\n",
      "\n",
      "Final Training Metrics:\n",
      "Loss: 0.0040\n",
      "Validation Loss: 0.0058\n",
      "MAE: 0.0337\n",
      "Validation MAE: 0.0576\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Set paths\n",
    "train_folder = \"D:\\\\PLL project\\\\data\\\\train\"  # Adjust this path\n",
    "model_save_path = \"D:\\\\PLL project\\\\LSTM\\\\Forcasting\\\\for_all_files\\\\Aman_code\\\\result\\\\/model.keras\"  # Adjust this path\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training process...\")\n",
    "model, history = train_model(train_folder, model_save_path)\n",
    "print(\"Training completed!\")\n",
    "\n",
    "# Print final metrics\n",
    "final_loss = history.history['loss'][-1]\n",
    "final_val_loss = history.history['val_loss'][-1]\n",
    "final_mae = history.history['mae'][-1]\n",
    "final_val_mae = history.history['val_mae'][-1]\n",
    "\n",
    "print(\"\\nFinal Training Metrics:\")\n",
    "print(f\"Loss: {final_loss:.4f}\")\n",
    "print(f\"Validation Loss: {final_val_loss:.4f}\")\n",
    "print(f\"MAE: {final_mae:.4f}\")\n",
    "print(f\"Validation MAE: {final_val_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fastN_fastP_3_185.csv...\n",
      "\u001b[1m8542/8542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step\n",
      "Processing fastN_slowP_3.6_185.csv...\n",
      "\u001b[1m8542/8542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 3ms/step\n",
      "Processing fastN_slowP_3_25.csv...\n",
      "\u001b[1m8542/8542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 3ms/step\n",
      "Processing slowN_slowP_3.6_185.csv...\n",
      "\u001b[1m8542/8542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 2ms/step\n",
      "Processing typical_3.6_185.csv...\n",
      "\u001b[1m8542/8542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 3ms/step\n",
      "Processing typical_3_5.csv...\n",
      "\u001b[1m8542/8542\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "def calculate_snr(signal, noise):\n",
    "    \"\"\"\n",
    "    Calculate the Signal-to-Noise Ratio (SNR) in dB.\n",
    "    \"\"\"\n",
    "    signal_power = np.mean(np.square(signal))\n",
    "    noise_power = np.mean(np.square(noise))\n",
    "    \n",
    "    if noise_power == 0:\n",
    "        return float('inf')  # Avoid division by zero\n",
    "    \n",
    "    return 10 * np.log10(signal_power / noise_power)\n",
    "\n",
    "def analyze_test_file(file_path, model_path, output_folder):\n",
    "    \"\"\"\n",
    "    Analyze a single test file, make predictions, and create visualizations\n",
    "    \"\"\"\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Split into actual values\n",
    "    actual_train = df['Pll_out'][:60000].values\n",
    "    actual_test = df['Pll_out'][60000:].values\n",
    "    \n",
    "    # Prepare data for prediction\n",
    "    X_test = actual_test.reshape(-1, 1, 1)\n",
    "    \n",
    "    # Load model and make predictions\n",
    "    model = load_model(model_path)\n",
    "    predictions = model.predict(X_test).flatten()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(actual_test, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(actual_test, predictions)\n",
    "    r2 = r2_score(actual_test, predictions)\n",
    "    snr = calculate_snr(actual_test, actual_test - predictions)\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Plot actual values (first 60000)\n",
    "    plt.plot(range(len(actual_train)), \n",
    "             actual_train, \n",
    "             label='Actual (First 60000)', \n",
    "             color='blue')\n",
    "    \n",
    "    # Plot predictions\n",
    "    plt.plot(range(len(actual_train), len(actual_train) + len(predictions)), \n",
    "             predictions, \n",
    "             label='Predictions', \n",
    "             color='red') \n",
    "             #, linestyle='--')\n",
    "    \n",
    "    # Add metrics to plot\n",
    "    metrics_text = f'Metrics (After 60000):\\nRMSE: {rmse:.2f}\\nMAE: {mae:.2f}\\nR²: {r2:.2f}\\nSNR: {snr:.2f} dB'\n",
    "    plt.text(0.02, 0.98, metrics_text,\n",
    "             transform=plt.gca().transAxes,\n",
    "             verticalalignment='top',\n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.title(f'Actual vs Predicted Values\\nFile: {os.path.basename(file_path)}')\n",
    "    plt.xlabel('Row Number')\n",
    "    plt.ylabel('Pll_out Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Save plot\n",
    "    plot_filename = os.path.join(output_folder, \n",
    "                                f'plot_{os.path.basename(file_path).replace(\".csv\", \".png\")}')\n",
    "    plt.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Return metrics\n",
    "    return {\n",
    "        'file': os.path.basename(file_path),\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2,\n",
    "        'snr': snr\n",
    "    }\n",
    "\n",
    "def process_all_test_files(test_folder, model_path, output_folder):\n",
    "    \"\"\"\n",
    "    Process all test files and save results\n",
    "    \"\"\"\n",
    "    # Create output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Get all CSV files\n",
    "    test_files = glob(os.path.join(test_folder, \"*.csv\"))\n",
    "    \n",
    "    # Process each file and collect metrics\n",
    "    all_metrics = []\n",
    "    for file in test_files:\n",
    "        print(f\"Processing {os.path.basename(file)}...\")\n",
    "        metrics = analyze_test_file(file, model_path, output_folder)\n",
    "        all_metrics.append(metrics)\n",
    "    \n",
    "    # Create metrics summary\n",
    "    metrics_df = pd.DataFrame(all_metrics)\n",
    "    metrics_filepath = os.path.join(output_folder, 'metrics_summary.csv')\n",
    "    metrics_df.to_csv(metrics_filepath, index=False)\n",
    "    \n",
    "    # Create summary plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    files = metrics_df['file']\n",
    "    x = range(len(files))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.bar(x, metrics_df['rmse'], alpha=0.6)\n",
    "    plt.title('RMSE by File')\n",
    "    plt.xticks(x, files, rotation=45)\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.bar(x, metrics_df['r2'], alpha=0.6)\n",
    "    plt.title('R² Score by File')\n",
    "    plt.xticks(x, files, rotation=45)\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.bar(x, metrics_df['snr'], alpha=0.6)\n",
    "    plt.title('SNR by File')\n",
    "    plt.xticks(x, files, rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_folder, 'metrics_summary.png'), \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Usage example for Colab\n",
    "if __name__ == \"__main__\":\n",
    "    # Define paths (adjust these for your Colab setup)\n",
    "    test_folder = \"D:\\\\PLL project\\\\data\\\\test\"\n",
    "    model_path = \"D:\\\\PLL project\\\\LSTM\\\\Forcasting\\\\for_all_files\\\\Aman_code\\\\result\\\\model.keras\"\n",
    "    output_folder = 'D:\\\\PLL project\\\\LSTM\\\\Forcasting\\\\for_all_files\\\\Aman_code\\\\result'\n",
    "    \n",
    "    # Process all test files\n",
    "    process_all_test_files(test_folder, model_path, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
